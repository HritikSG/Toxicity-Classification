# -*- coding: utf-8 -*-
"""new fea.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bv7EkdWXXW7SYMTT2q7A7_-aZpWSZaxE

# New Section
"""

import os
import pandas as pd
df=pd.read_csv("train.csv")
#GOOGLE_DRIVE_PATH_AFTER_MYDRIVE ="CIS-511-NLP/finalproject/train.csv"
#df = pd.read_csv(os.path.join('drive','MyDrive',GOOGLE_DRIVE_PATH_AFTER_MYDRIVE))
df.head()

df_filtered = df[["comment_text", "toxic"]].rename(columns={"comment_text": "comment"})

df_filtered.head()

# Filter toxic comments
toxic_comments = df_filtered[df_filtered["toxic"] == 1]
# Filter non-toxic comments
non_toxic_comments = df_filtered[df_filtered["toxic"] == 0]
# Display the counts
print(f"Number of toxic comments: {len(toxic_comments)}")
print(f"Number of non-toxic comments: {len(non_toxic_comments)}")

!pip install imblearn

from imblearn.under_sampling import RandomUnderSampler
# Undersample the majority class
undersampler = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = undersampler.fit_resample(df_filtered[['comment']], df_filtered['toxic'])
# Convert back to a DataFrame
df_resampled = pd.DataFrame({'comment': X_resampled['comment'], 'toxic': y_resampled})
# Display the class distribution
print(df_resampled['toxic'].value_counts())

import re

# Function to remove punctuation and newlines
def clean_text(text):
    # Remove punctuation and newline characters
    text = re.sub(r'[^\w\s]', '', text)  # Removes all characters except letters, numbers, and whitespace
    text = re.sub(r'\n', ' ', text)  # Replaces newline characters with a space
    return text.strip()

# Apply cleaning to the 'comment' column
df_resampled['comment'] = df_resampled['comment'].apply(clean_text)
corpus = df_resampled['comment'].values.tolist()



!pip install readability

!pip install textstat
!pip install textblob

from nltk.sentiment import SentimentIntensityAnalyzer
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from textstat import textstat  # Import textstat for readability features
from collections import Counter
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

nltk.download('vader_lexicon')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('averaged_perceptron_tagger')

# List of offensive words (extend as needed)
OFFENSIVE_WORDS = {"hate", "stupid", "idiot", "fool", "ugly"}

# Preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

# Feature extraction functions
def lexical_features(text):
    tokens = preprocess_text(text)

    return {
        "num_characters": len(text),
        "num_words": len(tokens),
        "num_sentences": len(sent_tokenize(text)),
        "avg_word_length": sum(len(word) for word in tokens) / len(tokens) if tokens else 0
    }
def sentiment(text):
    # Create the SentimentIntensityAnalyzer object
    sia = SentimentIntensityAnalyzer()

    # Get sentiment score for the text
    sentiment_score = sia.polarity_scores(text)['compound']

    # Convert sentiment score to numeric value
    if sentiment_score > 0:
        sentiment = 1  # Positive sentiment
    elif sentiment_score < 0:
        sentiment = -1  # Negative sentiment
    else:
        sentiment = 0  # Neutral sentiment

    # Return the sentiment as a numeric value
    return {
        "Sentiment": sentiment
    }
def syntactic_features(text):
    tokens = preprocess_text(text)
    pos_counts = Counter(tag for _, tag in pos_tag(tokens))
    return {
        "num_nouns": pos_counts.get('NN', 0),
        "num_verbs": pos_counts.get('VB', 0),
        "num_adjectives": pos_counts.get('JJ', 0),
    }

def domain_specific_features(text):
    tokens = preprocess_text(text)
    all_caps_words = [word for word in tokens if word.isupper()]
    toxic_keywords = [word for word in tokens if word in OFFENSIVE_WORDS]
    return {
        "num_all_caps_words": len(all_caps_words),
        "num_toxic_keywords": len(toxic_keywords)
    }

# Combine all features
def extract_features(text):
    features = {}
    features.update(lexical_features(text))
    features.update(syntactic_features(text))
    features.update(domain_specific_features(text))
    features.update(sentiment(text))
    return features

from tqdm import tqdm

batch_size = 1000
feature_set = []

for i in tqdm(range(0, len(corpus), batch_size), desc="Processing Batches"):
    batch = corpus[i:i + batch_size]
    batch_features = [extract_features(sample) for sample in batch]
    feature_set.extend(batch_features)

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
feature_set = np.array(feature_set)

# Compute TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit top terms
tfidf_features = tfidf_vectorizer.fit_transform(corpus)
print("Vocabulary size:", len(tfidf_vectorizer.vocabulary_))



import pickle
with open('tfidf_model.pkl', 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)

# import numpy as np
# from collections import Counter
# from itertools import combinations
# from nltk.tokenize import word_tokenize

# def compute_pmi_features(comments, vocabulary=None, window_size=5):
#     """
#     Compute PMI scores for frequent word pairs in a list of comments.

#     Parameters:
#     - comments: List of text strings (comments).
#     - vocabulary: List of words to consider for PMI (optional).
#     - window_size: Size of the sliding window for co-occurrence.

#     Returns:
#     - pmi_features: A 2D numpy array where each row corresponds to PMI features for a comment.
#     """
#     # Tokenize all comments and build a corpus-wide word frequency dictionary
#     tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]
#     word_counts = Counter(word for comment in tokenized_comments for word in comment)

#     # Define vocabulary if not provided
#     if vocabulary is None:
#         vocabulary = list(word_counts.keys())

#     # Initialize co-occurrence counts
#     co_occurrence_counts = Counter()

#     # Compute co-occurrences within a sliding window
#     for tokens in tokenized_comments:
#         for i in range(len(tokens) - window_size + 1):
#             window = tokens[i:i + window_size]
#             for word1, word2 in combinations(window, 2):  # Unique word pairs
#                 if word1 in vocabulary and word2 in vocabulary:
#                     co_occurrence_counts[(word1, word2)] += 1

#     # Calculate total number of sliding windows
#     total_windows = sum(len(tokens) - window_size + 1 for tokens in tokenized_comments if len(tokens) >= window_size)

#     # Compute PMI scores for all word pairs in the vocabulary
#     pmi_scores = {}
#     for (word1, word2), co_count in co_occurrence_counts.items():
#         if co_count > 0:
#             p_word1 = word_counts[word1] / sum(word_counts.values())
#             p_word2 = word_counts[word2] / sum(word_counts.values())
#             p_word1_word2 = co_count / total_windows
#             if p_word1 > 0 and p_word2 > 0 and p_word1_word2 > 0:
#                 pmi = np.log(p_word1_word2 / (p_word1 * p_word2))
#                 pmi_scores[(word1, word2)] = pmi

#     # Build feature vectors for each comment
#     feature_matrix = []
#     for tokens in tokenized_comments:
#         # Calculate average PMI for word pairs in the comment
#         comment_pmi = []
#         for word1, word2 in combinations(tokens, 2):
#             if (word1, word2) in pmi_scores:
#                 comment_pmi.append(pmi_scores[(word1, word2)])
#         # Use mean PMI for the comment (or 0 if no pairs have PMI scores)
#         avg_pmi = np.mean(comment_pmi) if comment_pmi else 0
#         feature_matrix.append([avg_pmi])

#     return np.array(feature_matrix)

# pmi_features = compute_pmi_features(corpus)

# np.save("feature_set.npy", np.asarray([list(f.values()) for f in feature_set]))

import numpy as np

n_samples = len(feature_set)
print(n_samples)
print(feature_set[0])


data = []

for i in range(n_samples):
  k = np.asarray([np.array(list(feature_set[i].values()), dtype=float)]).reshape(1, -1)
  j = tfidf_features[i].toarray()
  e = np.hstack((k, j))
  data.append(e.tolist())

data_np = np.asarray(data)
n_features = data_np.shape[-1]
print(n_features)

X = data_np.reshape(-1, n_features)
y = df_resampled['toxic'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)

!pip install transformers

!pip install xgboost
!pip install tensorflow

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from xgboost import XGBClassifier
import tensorflow as tf


model1 = RandomForestClassifier(n_estimators=100)
model2 = DecisionTreeClassifier()
model3 = LinearSVC()
model4 = XGBClassifier() #Nvidea implemented Classifier

# Define a simple deep learning model using Keras
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),  # Adjust input shape based on features
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification (use 'softmax' for multi-class)
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',  # Use 'categorical_crossentropy' for multi-class classification
              metrics=['accuracy'])

# Display model summary
model.summary()

model1.fit(X_train, y_train)
model2.fit(X_train, y_train)
model3.fit(X_train, y_train)
model4.fit(X_train, y_train)
# Train the model
model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))

with open('model.pkl', 'wb') as f:
    pickle.dump(model3, f)

from operator import mod
model1.score(X_train, y_train)
model2.score(X_train, y_train)
model3.score(X_train, y_train)
model4.score(X_train, y_train)
# Evaluate the model's performance on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy_Deeplearning: {test_accuracy * 100:.2f}%")



predictions1 = model1.predict(X_test)
predictions2 = model2.predict(X_test)
predictions3 = model3.predict(X_test)
predictions4 = model4.predict(X_test)
# Make predictions
predictions = model.predict(X_test)
predicted_labels = (predictions > 0.5).astype(int)  # Thresholding for binary classification

from sklearn.metrics import accuracy_score, confusion_matrix

acc1 = accuracy_score(y_test, predictions1)
print(f"Randon Forest Test accuracy: {acc1*100:.3f}")
acc2 = accuracy_score(y_test, predictions2)
print(f"DecisionTree Test accuracy: {acc2*100:.3f}")
acc3 = accuracy_score(y_test, predictions3)
print(f"Linear SVC Test accuracy: {acc3*100:.3f}")
acc4 = accuracy_score(y_test, predictions4)
print(f"XGB Test accuracy: {acc4*100:.3f}")

cfm1 = confusion_matrix(y_test, predictions1)
cfm2 = confusion_matrix(y_test, predictions2)
cfm3 = confusion_matrix(y_test, predictions3)
cfm4 = confusion_matrix(y_test, predictions4)
cfmdeep = confusion_matrix(y_test,predicted_labels)

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

disp1 = ConfusionMatrixDisplay(confusion_matrix=cfm1, display_labels=["not toxic", "toxic"])
disp1.plot(cmap='Blues')  # Optional: Use a color map
plt.title("Random Forest Confusion Matrix")
plt.show()

disp2 = ConfusionMatrixDisplay(confusion_matrix=cfm2, display_labels=["not toxic", "toxic"])
disp2.plot(cmap='Greens')  # Optional: Use a color map
plt.title("Decision tree Confusion Matrix")
plt.show()

disp3 = ConfusionMatrixDisplay(confusion_matrix=cfm3, display_labels=["not toxic", "toxic"])
disp3.plot(cmap='viridis')  # Optional: Use a color map
plt.title(" Linear SVC Confusion Matrix")
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

def evaluate_thresholds(y_true, y_probs, thresholds):
    """
    Evaluate and plot the confusion matrix for different thresholds.

    Args:
        y_true (list or np.array): True labels (0 or 1).
        y_probs (list or np.array): Predicted probabilities.
        thresholds (list): List of thresholds to evaluate.
    """
    for threshold in thresholds:
        # Convert probabilities to binary predictions
        y_pred = (y_probs >= threshold).astype(int)

        # Compute confusion matrix
        cfm4 = confusion_matrix(y_true, y_pred)

        # Print threshold and classification report
        print(f"\nThreshold: {threshold}")
        print(classification_report(y_true, y_pred, target_names=["not toxic", "toxic"]))

        # Display confusion matrix
        disp = ConfusionMatrixDisplay(confusion_matrix=cfm4, display_labels=["not toxic", "toxic"])
        disp.plot(cmap='Blues')  # Optional: Use a color map
        plt.title(f"Confusion Matrix (Threshold: {threshold})")
        plt.show()

# Example predicted probabilities (replace with your model's predictions)
# Ensure `model.predict_proba` returns probabilities for the positive class
y_probs = model4.predict_proba(X_test)[:, 1]
thresholds = [0.1,0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]

# Call the function
evaluate_thresholds(y_test, y_probs, thresholds)

dispdeep = ConfusionMatrixDisplay(confusion_matrix=cfmdeep, display_labels=["not toxic", "toxic"])
dispdeep.plot(cmap='Reds')  # Optional: Use a color map
plt.title("Deep Learning Confusion Matrix")
plt.show()